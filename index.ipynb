{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Gaussian Naive Bayes\n", "\n", "## Introduction\n", "\n", "Expanding Bayes theorem to account for multiple observations and conditional probabilities drastically increases predictive power. In essence, it allows you to develop a belief network taking into account all of the available information regarding the scenario. In this lesson, you'll take a look at one particular implementation of a multinomial naive Bayes algorithm: Gaussian Naive Bayes.\n", "\n", "## Objectives\n", "\n", "You will be able to:\n", "\n", "* Explain the Gaussian Naive Bayes algorithm\n", "* Implement the Gaussian Naive Bayes (GNB) algorithm using SciPy and NumPy"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Theoretical background\n", "\n", "Multinomial Bayes expands upon Bayes' theorem to multiple observations.\n", "\n", "Recall that Bayes' theorem is:  \n", "\n", "$$ \\Large P(A|B) = \\frac{P(B|A)\\bullet P(A)}{P(B)}$$\n", "\n", "Expanding to multiple features, the multinomial Bayes' formula is:  \n", "\n", "$$ \\Large P(y|x_1, x_2, ..., x_n) = \\frac{P(y)\\prod_{i}^{n}P(x_i|y)}{P(x_1, x_2, ..., x_n)}$$\n", "\n", "\n", "Here $y$ is an observation class while $x_1$ through $x_n$ are various features of the observation. Similar to linear regression, these features are assumed to be linearly independent. The motivating idea is that the various features $x_1, x_2,...x_n$ will help inform which class a particular observation belongs to. This could be anything from 'Does this person have a disease?' to 'Is this credit card purchase fraudulent' or 'What marketing audience does this individual fall into?'. In this lesson you will work with classic iris dataset. This dataset includes various measurements of a flower's anatomy and the specific species of the flower. For that dataset, $y$ would be the flower species while $x_1$ through $x_n$ would be the various measurements for a given flower. As such, the equation for Multinomial Bayes, given above, would allow you to calculate the probability that a given flower belongs to a specific category of species.\n", "\n", "With that, let's dig into the formula a little more to get a deeper understanding. In the numerator, you multiply the product of the conditional probabilities $P(x_i|y)$ by the probability of the class y. The denominator is the overall probability (across all classes) for the observed values of the various features. In practice, this can be difficult or impossible to calculate. Fortunately, doing so is typically not required, as you will simply be comparing the relative probabilities of the various classes&mdash;do you believe this flower is of species A, species B or species C?  \n", "\n", "To calculate each of the conditional probabilities in the numerator, $P(x_i|y)$, the Gaussian Naive Bayes algorithm traditionally uses the Gaussian probability density function to give a relative estimate of the probability of the feature observation, $x_i$, for the class $y$. Some statisticians don't agree with this as the probability of any point on a PDF curve is actually 0. As you've seen in z-tests and t-tests, only ranges of values have a probability, and these are calculated by taking the area under the PDF curve for the given range. While true, these point estimates can be loosely used as 'the relative probability for values near $x_i$'. \n", "\n", "With that, you have:  \n", "\n", "$$\\Large P(x_i|y) = \\frac{1}{\\sqrt{2\\pi \\sigma_i^2}}e^{\\frac{-(x-\\mu_i)^2}{2\\sigma_i^2}}$$\n", "\n", "Where $\\mu_i$ is the mean of feature $x_i$ for class $y$ and $\\sigma_i^2$ is the variance of feature $x_i$ for class $y$.\n", "\n", "From there, each of the relative posterior probabilities are calculated for each of the classes. The largest of these is the class which is the most probable for the given observation.  \n", "\n", "With that, let's take a look in practice to try to make this process a little clearer."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Load the dataset\n", "\n", "First, let's load in the Iris dataset to use to demonstrate the Gaussian Naive Bayes algorithm: "]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>sepal length (cm)</th>\n", "      <th>sepal width (cm)</th>\n", "      <th>petal length (cm)</th>\n", "      <th>petal width (cm)</th>\n", "      <th>Target</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>5.1</td>\n", "      <td>3.5</td>\n", "      <td>1.4</td>\n", "      <td>0.2</td>\n", "      <td>0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>4.9</td>\n", "      <td>3.0</td>\n", "      <td>1.4</td>\n", "      <td>0.2</td>\n", "      <td>0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>4.7</td>\n", "      <td>3.2</td>\n", "      <td>1.3</td>\n", "      <td>0.2</td>\n", "      <td>0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>4.6</td>\n", "      <td>3.1</td>\n", "      <td>1.5</td>\n", "      <td>0.2</td>\n", "      <td>0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>5.0</td>\n", "      <td>3.6</td>\n", "      <td>1.4</td>\n", "      <td>0.2</td>\n", "      <td>0</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n", "0                5.1               3.5                1.4               0.2   \n", "1                4.9               3.0                1.4               0.2   \n", "2                4.7               3.2                1.3               0.2   \n", "3                4.6               3.1                1.5               0.2   \n", "4                5.0               3.6                1.4               0.2   \n", "\n", "   Target  \n", "0       0  \n", "1       0  \n", "2       0  \n", "3       0  \n", "4       0  "]}, "execution_count": 1, "metadata": {}, "output_type": "execute_result"}], "source": ["from sklearn import datasets\n", "import pandas as pd\n", "import numpy as np\n", "\n", "iris = datasets.load_iris()\n", "\n", "X = pd.DataFrame(iris.data)\n", "X.columns = iris.feature_names\n", "\n", "y = pd.DataFrame(iris.target)\n", "y.columns = ['Target']\n", "\n", "df = pd.concat([X, y], axis=1)\n", "df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["It's always a good idea to briefly examine the data. In this case, let's check how many observations there are for each flower species:"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"data": {"text/plain": ["2    50\n", "1    50\n", "0    50\n", "Name: Target, dtype: int64"]}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": ["df['Target'].value_counts()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Calculate the mean and standard deviation of each feature for each class\n", "\n", "Next, you calculate the mean and standard deviation within a class for each of the features. You'll then use these values to calculate the conditional probability of a particular feature observation for each of the classes."]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead tr th {\n", "        text-align: left;\n", "    }\n", "\n", "    .dataframe thead tr:last-of-type th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr>\n", "      <th></th>\n", "      <th colspan=\"2\" halign=\"left\">sepal length (cm)</th>\n", "      <th colspan=\"2\" halign=\"left\">sepal width (cm)</th>\n", "      <th colspan=\"2\" halign=\"left\">petal length (cm)</th>\n", "      <th colspan=\"2\" halign=\"left\">petal width (cm)</th>\n", "    </tr>\n", "    <tr>\n", "      <th></th>\n", "      <th>mean</th>\n", "      <th>std</th>\n", "      <th>mean</th>\n", "      <th>std</th>\n", "      <th>mean</th>\n", "      <th>std</th>\n", "      <th>mean</th>\n", "      <th>std</th>\n", "    </tr>\n", "    <tr>\n", "      <th>Target</th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>5.006</td>\n", "      <td>0.352490</td>\n", "      <td>3.428</td>\n", "      <td>0.379064</td>\n", "      <td>1.462</td>\n", "      <td>0.173664</td>\n", "      <td>0.246</td>\n", "      <td>0.105386</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>5.936</td>\n", "      <td>0.516171</td>\n", "      <td>2.770</td>\n", "      <td>0.313798</td>\n", "      <td>4.260</td>\n", "      <td>0.469911</td>\n", "      <td>1.326</td>\n", "      <td>0.197753</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>6.588</td>\n", "      <td>0.635880</td>\n", "      <td>2.974</td>\n", "      <td>0.322497</td>\n", "      <td>5.552</td>\n", "      <td>0.551895</td>\n", "      <td>2.026</td>\n", "      <td>0.274650</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["       sepal length (cm)           sepal width (cm)            \\\n", "                    mean       std             mean       std   \n", "Target                                                          \n", "0                  5.006  0.352490            3.428  0.379064   \n", "1                  5.936  0.516171            2.770  0.313798   \n", "2                  6.588  0.635880            2.974  0.322497   \n", "\n", "       petal length (cm)           petal width (cm)            \n", "                    mean       std             mean       std  \n", "Target                                                         \n", "0                  1.462  0.173664            0.246  0.105386  \n", "1                  4.260  0.469911            1.326  0.197753  \n", "2                  5.552  0.551895            2.026  0.274650  "]}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": ["aggs = df.groupby('Target').agg(['mean', 'std'])\n", "aggs"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Calculate conditional probability point estimates\n", "\n", "Take another look at how to implement point estimates for the conditional probabilities of a feature for a given class. To do this, you'll simply use the PDF of the normal distribution. (Again, there can be some objection to this method as the probability of a specific point for a continuous distribution is 0. Some statisticians bin the continuous distribution into a discrete approximation to remedy this, but doing so requires additional work and the width of these bins is an arbitrary value which will potentially impact results.)\n", "\n", "$$ \\Large P(x_i|y) = \\frac{1}{\\sqrt{2 \\pi \\sigma_i^2}}e^{\\frac{-(x-\\mu_i)^2}{2\\sigma_i^2}}$$\n"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"data": {"text/plain": ["2.1553774365786804"]}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": ["from scipy import stats\n", "\n", "def p_x_given_class(obs_row, feature, class_):\n", "    mu = aggs[feature]['mean'][class_]\n", "    std = aggs[feature]['std'][class_]\n", "    \n", "    # A single observation\n", "    obs = df.iloc[obs_row][feature] \n", "    \n", "    p_x_given_y = stats.norm.pdf(obs, loc=mu, scale=std)\n", "    return p_x_given_y\n", "\n", "# Notice how this is not a true probability; you can get values > 1\n", "p_x_given_class(0, 'petal length (cm)', 0) "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Multinomial Bayes"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"data": {"text/plain": ["[0.0004469582872647558,\n", " 0.00044432855867026464,\n", " 5.436807559640758e-152,\n", " 9.529514999027405e-251,\n", " 0.20091323410933296,\n", " 0.06135077392562668,\n", " 5.488088968636944e-05,\n", " 2.460149009916488e-12,\n", " 0.1887425821931875,\n", " 0.140076102721696,\n", " 0.0728335779635225,\n", " 0.023861042537402642]"]}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": ["row = 100\n", "c_probs = []\n", "for c in range(3):\n", "    # Initialize probability to relative probability of class \n", "    p = len(df[df['Target'] == c])/len(df) \n", "    for feature in X.columns:\n", "        p *= p_x_given_class(row, feature, c) \n", "        # Update the probability using the point estimate for each feature\n", "        c_probs.append(p)\n", "\n", "c_probs"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Calculating class probabilities for observations\n", "\n", "While you haven't even attempted to calculate the denominator for the original equation,  \n", "\n", "$$P(y|x_1,x_2,...x_n) = \\frac{P(y)\\prod_{i}^{n}P(x_i|y)}{P(x_1,x_2,...x_n)}$$ \n", "\n", "you don't really have to.  \n", "\n", "That is, the probability $P(x_1, x_2, ..., x_n)$ is the probability of the given observation across all classes; it is not a function of class at all. As such, it will be a constant across all of these posterior class probabilities. Since you are simply interested in the most likely class for each observation, you can simply pick the class with the largest numerator. With that, let's adapt the code snippet above to create a function which predicts a class for a given row of data."]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["def predict_class(row):\n", "    c_probs = []\n", "    for c in range(3):\n", "        # Initialize probability to relative probability of class\n", "        p = len(df[df['Target'] == c])/len(df) \n", "        for feature in X.columns:\n", "            p *= p_x_given_class(row, feature, c)\n", "        c_probs.append(p)\n", "    return np.argmax(c_probs)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's also take an example row to test this new function: "]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"data": {"text/plain": ["sepal length (cm)    5.1\n", "sepal width (cm)     3.5\n", "petal length (cm)    1.4\n", "petal width (cm)     0.2\n", "Target               0.0\n", "Name: 0, dtype: float64"]}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": ["row = 0\n", "df.iloc[row]"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"data": {"text/plain": ["0"]}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": ["predict_class(row)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Nice! It appears that this `predict_class()` function has correctly predicted the class for this first row! Now it's time to take a look at how accurate this function is across the entire dataset!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Calculating accuracy\n", "\n", "In order to determine the overall accuracy of your newly minted Gaussian Naive Bayes classifier, you'll need to generate predictions for all of the rows in the dataset. From there, you can then compare these predictions to the actual class values stored in the 'Target' column. Take a look:"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"data": {"text/plain": ["True     0.96\n", "False    0.04\n", "Name: Correct?, dtype: float64"]}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": ["df['Predictions'] =  [predict_class(row) for row in df.index]\n", "df['Correct?'] = df['Target'] == df['Predictions']\n", "df['Correct?'].value_counts(normalize=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Summary\n", "\n", "Nicely done! You're well on your way to using Bayesian statistics in the context of machine learning! In this lesson, you saw how to adapt Bayes theorem along with your knowledge of the normal distribution to create a machine learning classifier known as Gaussian Naive Bayes. "]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.3"}}, "nbformat": 4, "nbformat_minor": 2}